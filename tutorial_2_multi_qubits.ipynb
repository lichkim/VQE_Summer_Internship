{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lichkim/VQE_Summer_Internship/blob/master/tutorial_2_multi_qubits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGYND9QwYQWS",
        "outputId": "4a961f5f-2c7e-4da8-f8dc-b1c8ca4d2aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: scipy<=1.10 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.28.1)\n",
            "Requirement already satisfied: autograd<=1.5 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4)\n",
            "Requirement already satisfied: pennylane-lightning>=0.31 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.31.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.2.4)\n",
            "Requirement already satisfied: numpy<1.24 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.22.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.3.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.8.5)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.13.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd<=1.5->pennylane) (0.18.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (1.26.11)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2022.6.15)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-01 09:08:02.340429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pennylane\n",
        "import matplotlib.pyplot as plt\n",
        "from pennylane import numpy as np\n",
        "import pennylane as qml\n",
        "import tensorflow as tf\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: PennyLane\n",
            "Version: 0.31.0\n",
            "Summary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\n",
            "Home-page: https://github.com/PennyLaneAI/pennylane\n",
            "Author: \n",
            "Author-email: \n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml\n",
            "Required-by: PennyLane-Cirq, PennyLane-Lightning, PennyLane-qiskit, PennyLane-SF\n",
            "\n",
            "Platform info:           Linux-5.15.0-78-generic-x86_64-with-glibc2.35\n",
            "Python version:          3.10.4\n",
            "Numpy version:           1.22.4\n",
            "Scipy version:           1.9.0\n",
            "Installed devices:\n",
            "- lightning.qubit (PennyLane-Lightning-0.31.0)\n",
            "- default.gaussian (PennyLane-0.31.0)\n",
            "- default.mixed (PennyLane-0.31.0)\n",
            "- default.qubit (PennyLane-0.31.0)\n",
            "- default.qubit.autograd (PennyLane-0.31.0)\n",
            "- default.qubit.jax (PennyLane-0.31.0)\n",
            "- default.qubit.tf (PennyLane-0.31.0)\n",
            "- default.qubit.torch (PennyLane-0.31.0)\n",
            "- default.qutrit (PennyLane-0.31.0)\n",
            "- null.qubit (PennyLane-0.31.0)\n",
            "- qiskit.aer (PennyLane-qiskit-0.24.0)\n",
            "- qiskit.basicaer (PennyLane-qiskit-0.24.0)\n",
            "- qiskit.ibmq (PennyLane-qiskit-0.24.0)\n",
            "- qiskit.ibmq.circuit_runner (PennyLane-qiskit-0.24.0)\n",
            "- qiskit.ibmq.sampler (PennyLane-qiskit-0.24.0)\n",
            "- cirq.mixedsimulator (PennyLane-Cirq-0.24.0)\n",
            "- cirq.pasqal (PennyLane-Cirq-0.24.0)\n",
            "- cirq.qsim (PennyLane-Cirq-0.24.0)\n",
            "- cirq.qsimh (PennyLane-Cirq-0.24.0)\n",
            "- cirq.simulator (PennyLane-Cirq-0.24.0)\n",
            "- strawberryfields.fock (PennyLane-SF-0.20.1)\n",
            "- strawberryfields.gaussian (PennyLane-SF-0.20.1)\n",
            "- strawberryfields.gbs (PennyLane-SF-0.20.1)\n",
            "- strawberryfields.remote (PennyLane-SF-0.20.1)\n",
            "- strawberryfields.tf (PennyLane-SF-0.20.1)\n"
          ]
        }
      ],
      "source": [
        "qml.about()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.12.0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install tensorflow=='2.12'\n",
        "tf.__version__  #tensorflow version이 2.13.0이면 문제 발생하니까 2.12로 바꿔서 설치하세요\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qvR94D6sils3"
      },
      "outputs": [],
      "source": [
        "class Sigma:\n",
        "    def __init__(self, *args):\n",
        "            '''\n",
        "            Sigma class generates algebraic primitives required to compute the system of implicit ODEs for Euler angles.\n",
        "            '''\n",
        "            self.string = [i for i in args]\n",
        "            self.order = len(args)\n",
        "            self.digit = sum([args[i]*4**(self.order-i-1) for i in range(len(args))])\n",
        "            self.pauli = tf.constant([\n",
        "                 [[-1j/2,0],[0,-1j/2]],[[0, 1j/2],[1j/2, 0]],[[0, -1/2],[1/2, 0]],[[1j/2, 0],[0, -1j/2]]\n",
        "                      ],dtype=tf.complex64,shape=(4,2,2))\n",
        "            self.__struct_const_0 = tf.constant(\n",
        "                [\n",
        "                    np.zeros([4,4]),\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, -1.0],[0.0, 0.0, 1.0, 0.0]],\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 1.0],[0.0, 0.0, 0.0, 0.0],[0.0, -1.0, 0.0, 0.0]],\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, -1.0, 0.0],[0.0, 1.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]\n",
        "                    ], dtype=tf.complex64\n",
        "            )\n",
        "            self.__struct_const_1 = tf.constant(\n",
        "                [\n",
        "                    -1j*np.eye(4),\n",
        "                    -1j*np.array([[0.0, 1.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]),\n",
        "                    -1j*np.array([[0.0, 0.0, 1.0, 0.0],[0.0, 0.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]),\n",
        "                    -1j*np.array([[0.0, 0.0, 0.0, 1.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0]])\n",
        "                    ], dtype=tf.complex64\n",
        "            )\n",
        "            self.__vector = tf.constant([\n",
        "                [1,0,0,0],\n",
        "                [0,1,0,0],\n",
        "                [0,0,1,0],\n",
        "                [0,0,0,1]], dtype=tf.float32\n",
        "            )\n",
        "            self.index_perm = list(range(0,self.order*2,2)) + list(range(1,self.order*2+1,2))\n",
        "            self.fundamental = self.rep_fundamental()\n",
        "            self.adjoint = self.rep_adjoint()\n",
        "            self.fundamental_z = tf.eye(2,dtype=tf.complex64)\n",
        "            for i in range(1,self.order):\n",
        "                self.fundamental_z = tf.tensordot(self.fundamental_z,tf.eye(2,dtype=tf.complex64),axes=0)\n",
        "            self.fundamental_z = tf.transpose(self.fundamental_z,perm=self.index_perm)\n",
        "            self.adjoint_z = tf.eye(4,dtype=tf.float32)\n",
        "            for i in range(1,self.order):\n",
        "                self.adjoint_z = tf.tensordot(self.adjoint_z,tf.eye(4,dtype=tf.float32),axes=0)\n",
        "            self.adjoint_z = tf.transpose(self.adjoint_z,perm=self.index_perm)\n",
        "            self.adjoint_square = tf.tensordot(self.adjoint,self.adjoint,axes=self.order)\n",
        "            self.adj_vec = self.adj_vector()\n",
        "\n",
        "    def rep_fundamental(self):\n",
        "        index = self.string\n",
        "        ret = self.pauli[index[0]]\n",
        "        for i in range(1,len(index)):\n",
        "            ret = 2j*tf.tensordot(ret,self.pauli[index[i]],axes=0)\n",
        "        return tf.transpose(ret,perm=self.index_perm)\n",
        "\n",
        "\n",
        "    def rep_adjoint(self):\n",
        "        return tf.transpose(tf.cast(self.__rep_adjoint(self.string),dtype=tf.float32),perm=self.index_perm)\n",
        "\n",
        "    def __rep_adjoint(self,sigma,center=False,grade=False):\n",
        "        order = len(sigma)\n",
        "        if grade:\n",
        "            ret = self.__struct_const_1[sigma[0]]\n",
        "        else:\n",
        "            ret = self.__struct_const_0[sigma[0]]\n",
        "\n",
        "        if order > 1:\n",
        "            if grade:\n",
        "                ret = 1.j* (\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=True),  self.__rep_adjoint([sigma[order-1]],center=True,grade=True),axes=0) +\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=False), self.__rep_adjoint([sigma[order-1]],center=True,grade=False),axes=0)\n",
        "                    )\n",
        "            else:\n",
        "                ret = 1.j* (\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=True),  self.__rep_adjoint([sigma[order-1]],center=True,grade=False),axes=0) +\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=False), self.__rep_adjoint([sigma[order-1]],center=True,grade=True),axes=0)\n",
        "                    )\n",
        "\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def adj_vector(self):\n",
        "        index = self.string\n",
        "        ret = self.__vector[index[0]]\n",
        "        for i in range(1,len(index)):\n",
        "            ret = tf.tensordot(ret,self.__vector[index[i]],axes=0)\n",
        "        return ret\n",
        "    @tf.function\n",
        "    def exp_fundamental(self,x):\n",
        "        s = tf.cast(tf.math.sin(x/2),tf.complex64)\n",
        "        c = tf.cast(tf.math.cos(x/2),tf.complex64)\n",
        "        return tf.tensordot(c, self.fundamental_z,axes=0) + tf.tensordot(2*s,  self.fundamental,axes=0)\n",
        "    @tf.function\n",
        "    def exp_adjoint(self,x):\n",
        "        return self.adjoint_z + tf.tensordot(tf.math.sin(x), self.adjoint,axes=0) + tf.tensordot((1 - tf.math.cos(x)), self.adjoint_square,axes=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ObzmIg8Od0zT"
      },
      "outputs": [],
      "source": [
        "class Euler():\n",
        "    def __init__(self, order):\n",
        "        self.order = order\n",
        "        self.basis = np.array(list(product([0,1,2,3], repeat = order)))\n",
        "        self.toral = np.array(list(product([0,3], repeat = order)))\n",
        "        self.kprime = self.__kprime(order)\n",
        "        self.aprime = self.__aprime(order)\n",
        "        self.k = self.__k(order)\n",
        "        self.a = self.__a(order)\n",
        "        self.kak = np.concatenate((self.k,self.a,self.k),axis=0)\n",
        "        self.generator = self.__generator(order)\n",
        "        self.dim = len(self.kak)\n",
        "\n",
        "    def __kak(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[3],[1],[3]])\n",
        "        else:\n",
        "            # return self.__a(order-1)\n",
        "            return np.concatenate((self.__k(order),self.__a(order),self.__k(order)),axis=0)\n",
        "\n",
        "    def __kprime(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([],dtype=np.int32)\n",
        "        else:\n",
        "            # print(self.__kak(order-1))\n",
        "            return np.insert(self.__kak(order-1),0,0,axis=1)\n",
        "\n",
        "    def __aprime(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([],dtype=np.int32)\n",
        "        else:\n",
        "            return np.insert(np.array(list(product([0,3], repeat = order-1))),0,3,axis=1)\n",
        "\n",
        "    def __k(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[3]])\n",
        "        else:\n",
        "            # print(self.kprime)\n",
        "            # return np.concatenate((self.kprime,self.aprime,self.kprime),axis=0)\n",
        "            return np.concatenate((self.__kprime(order),self.__aprime(order),self.__kprime(order)),axis=0)\n",
        "\n",
        "    def __a(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[1]])\n",
        "        else:\n",
        "            return np.insert(np.array(list(product([0,3], repeat = order-1))),0,1,axis=1)\n",
        "\n",
        "    def __generator(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[1],[3]])\n",
        "        else:\n",
        "            ret = np.insert(self.__generator(order-1),0,0,axis=1)\n",
        "            return np.concatenate((ret,self.__a(order),self.__aprime(order)),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MxKRdo80Ye2o"
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=1)\n",
        "def circuit(params, wires=0):\n",
        "    qml.RX(params[0], wires=wires)\n",
        "    qml.RY(params[1], wires=wires) #ansatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### qml.Hamiltonian: Hamiltonian은 다른 operator들의 linear combination으로 구성됩니다. 즉, $\\sum_{k=0}^{n-1} c_{k}O_{k}$입니다. 여기서 $c_{k}$는 훈련시킬 수 있는 parameter입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9k-Z75JiYuHB"
      },
      "outputs": [],
      "source": [
        "coeffs = [1, 1]\n",
        "obs = [qml.PauliX(0), qml.PauliZ(0)]\n",
        "\n",
        "H = qml.Hamiltonian(coeffs, obs)  #define Hamiltonian\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\") #parameter shift\n",
        "def cost_fn(params):    #정의한 hamiltonian에 대한 기댓값을 구합니다.\n",
        "    circuit(params)\n",
        "    return qml.expval(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z8QY7BtlYy-V"
      },
      "outputs": [],
      "source": [
        "init_params = np.array([3.97507603, 3.00854038], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "crxFYAkaYzj2"
      },
      "outputs": [],
      "source": [
        "max_iterations = 500\n",
        "conv_tol = 1e-06\n",
        "#step_size = 0.1 #learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pennylane의 GradientDescentOptimizer 클래스를 불러와 최적화 하기\n",
        "\n",
        "##### GradientDescentOptimzer의 [document](https://docs.pennylane.ai/en/stable/code/api/pennylane.GradientDescentOptimizer.html)\n",
        "\n",
        "##### 일반적인 GradientDescent 방식으로 최적화합니다. 아래의 식을 따릅니다. $$x^{\\left(t+1\\right)} = x^{\\left(t\\right)} - \\eta\\nabla f\\left(x^{\\left(t\\right)}\\right)$$ 여기서 $\\eta$는 유저가 인자 stepsize를 통해 정의할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ZEmFIjY23g",
        "outputId": "4dac2c7c-b471-4758-9a19-d6ba1fc4089f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration = 0,  Energy = 0.48118828 Ha,  Convergence parameter = 0.09598332 Ha\n",
            "Iteration = 20,  Energy = -0.46381708 Ha,  Convergence parameter = 0.08210876 Ha\n",
            "Iteration = 40,  Energy = -1.40651648 Ha,  Convergence parameter = 0.00272091 Ha\n",
            "Iteration = 60,  Energy = -1.41419617 Ha,  Convergence parameter = 0.00000620 Ha\n",
            "\n",
            "Final value of the energy = -1.41421077 Ha\n",
            "Number of iterations =  66\n"
          ]
        }
      ],
      "source": [
        "#opt = qml.GradientDescentOptimizer(stepsize=step_size)\n",
        "opt = qml.GradientDescentOptimizer(stepsize=0.1)\n",
        "\n",
        "params = init_params\n",
        "\n",
        "gd_param_history = [params]\n",
        "gd_cost_history = []\n",
        "\n",
        "for n in range(max_iterations):\n",
        "\n",
        "    # Take step\n",
        "    params, prev_energy = opt.step_and_cost(cost_fn, params)\n",
        "    gd_param_history.append(params)\n",
        "    gd_cost_history.append(prev_energy)\n",
        "\n",
        "    energy = cost_fn(params)\n",
        "\n",
        "    # Calculate difference between new and old energies\n",
        "    conv = np.abs(energy - prev_energy)\n",
        "\n",
        "    if n % 20 == 0:\n",
        "        print(\n",
        "            \"Iteration = {:},  Energy = {:.8f} Ha,  Convergence parameter = {\"\n",
        "            \":.8f} Ha\".format(n, energy, conv)\n",
        "        )\n",
        "\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "\n",
        "print()\n",
        "print(\"Final value of the energy = {:.8f} Ha\".format(energy))\n",
        "print(\"Number of iterations = \", n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pennylane의 QNGOptimizer 클래스를 불러와 최적화하기\n",
        "\n",
        "##### QNGOptimzer의 [document](https://docs.pennylane.ai/en/stable/code/api/pennylane.QNGOptimizer.html)\n",
        "\n",
        "##### Fubini-Study metric tensor에 diagonal 혹은 block-diagonal 근사를 하여 learning rate를 지속적으로 수정하는 optimizer입니다.\n",
        "\n",
        "##### QNG optimizer는 Fubini-Study metric tensor $g$의 pseudo-inverse에 기반한 step과 parameter에 영향을 받는 learning rate를 사용합니다. $$x^{\\left(t+1\\right)} = x^{\\left(t\\right)} - \\eta g\\left(f\\left(x^{\\left(t\\right)}\\right)\\right)^{-1} \\nabla f\\left(x^{\\left(t\\right)}\\right)$$ 여기서 $f\\left(x^{\\left(t\\right)}\\right) = \\bra{0}U\\left(x^{\\left(t\\right)}\\right)^{\\dagger}\\hat{B}U\\left(x^{\\left(t\\right)}\\right)\\ket{0}$는 variational quantum circuit $U\\left(x^{\\left(t\\right)}\\right)$에서 측정된 observable의 기댓값입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Variational Quantum Circuit으로 표현된 Quantum Node를 생각해봅시다. $$ U\\left(\\theta\\right) = W\\left(\\theta_{i+1},\\cdots,\\theta_{N}\\right)X\\left(\\theta_{i}\\right)V\\left(\\theta_{1},\\cdots,\\theta_{i-1}\\right)$$ 여기서 모든 parametrized gate들은 $X\\left(\\theta_{i}\\right) = e^{i\\theta_{i}K_{i}}$로 표현될 수 있습니다.<br>\n",
        "\n",
        "##### 즉, 게이트 $K_{i}$가 $i$번째 파라미터에 대응하는 parameterized operation $X\\left(\\theta_{i}\\right)$의 *generator*입니다.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRqoP5pzY6YO",
        "outputId": "f9accdef-8711-4c37-b00b-0226142b6b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration = 0,  Energy = -0.01070497 Ha,  Convergence parameter = 0.58787657 Ha\n",
            "\n",
            "Final value of the energy = -1.41421347 Ha\n",
            "Number of iterations =  11\n"
          ]
        }
      ],
      "source": [
        "#opt = qml.QNGOptimizer(stepsize=step_size, approx=\"block-diag\")\n",
        "opt = qml.QNGOptimizer(stepsize=0.1, approx=\"block-diag\")\n",
        "\n",
        "params = init_params\n",
        "\n",
        "qngd_param_history = [params]\n",
        "qngd_cost_history = []\n",
        "\n",
        "for n in range(max_iterations):\n",
        "\n",
        "    # Take step\n",
        "    params, prev_energy = opt.step_and_cost(cost_fn, params)\n",
        "    qngd_param_history.append(params)\n",
        "    qngd_cost_history.append(prev_energy)\n",
        "\n",
        "    # Compute energy\n",
        "    energy = cost_fn(params)\n",
        "\n",
        "    # Calculate difference between new and old energies\n",
        "    conv = np.abs(energy - prev_energy)\n",
        "\n",
        "    if n % 20 == 0:\n",
        "        print(\n",
        "            \"Iteration = {:},  Energy = {:.8f} Ha,  Convergence parameter = {\"\n",
        "            \":.8f} Ha\".format(n, energy, conv)\n",
        "        )\n",
        "\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "\n",
        "print()\n",
        "print(\"Final value of the energy = {:.8f} Ha\".format(energy))\n",
        "print(\"Number of iterations = \", n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kGf0-JjaZBIV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-01 09:08:07.585337: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "@qml.qnode(dev, interface=\"tf\")\n",
        "def tf_cost_fn(params):\n",
        "    circuit(params)\n",
        "    return qml.expval(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "seWLaPp3b7WN"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "n_neuron = 8 #hidden layer\n",
        "\n",
        "nn_vars = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((1,)), #input neuron 1\n",
        "    tf.keras.layers.Dense(n_neuron, activation='tanh'),\n",
        "    tf.keras.layers.Dense(2) #output\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tensorflow를 이용해 Classical하게 최적화하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpdQ-ZRVcKRE",
        "outputId": "ce9d0e71-6133-4512-ba6d-347458cfcb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 0.2851590689849563, convergence 0.2851590689849563\n",
            "epoch: 2, loss: 0.2026917338724838, convergence 0.0824673351124725\n",
            "epoch: 3, loss: 0.12866993852360026, convergence 0.07402179534888353\n",
            "epoch: 4, loss: 0.0643721334287164, convergence 0.06429780509488386\n",
            "epoch: 5, loss: 0.010237663373732497, convergence 0.054134470054983896\n",
            "epoch: 6, loss: -0.03480051402874046, convergence 0.045038177402472956\n",
            "epoch: 7, loss: -0.07383163390468395, convergence 0.03903111987594349\n",
            "epoch: 8, loss: -0.11170225460256533, convergence 0.037870620697881385\n",
            "epoch: 9, loss: -0.15363880123203671, convergence 0.04193654662947138\n",
            "epoch: 10, loss: -0.20361567042457596, convergence 0.04997686919253924\n",
            "epoch: 11, loss: -0.26392321418429854, convergence 0.06030754375972258\n",
            "epoch: 12, loss: -0.33550113983948027, convergence 0.07157792565518173\n",
            "epoch: 13, loss: -0.41824660603410335, convergence 0.08274546619462309\n",
            "epoch: 14, loss: -0.511175590476344, convergence 0.09292898444224063\n",
            "epoch: 15, loss: -0.6125141981255321, convergence 0.10133860764918812\n",
            "epoch: 16, loss: -0.7197849074021202, convergence 0.1072707092765881\n",
            "epoch: 17, loss: -0.8299123831127451, convergence 0.11012747571062487\n",
            "epoch: 18, loss: -0.9393685141405723, convergence 0.10945613102782725\n",
            "epoch: 19, loss: -1.0443646373165418, convergence 0.10499612317596951\n",
            "epoch: 20, loss: -1.1410929566650472, convergence 0.09672831934850534\n",
            "epoch: 21, loss: -1.2260136028492852, convergence 0.084920646184238\n",
            "epoch: 22, loss: -1.2961759291822665, convergence 0.07016232633298136\n",
            "epoch: 23, loss: -1.349545275583715, convergence 0.053369346401448414\n",
            "epoch: 24, loss: -1.385299359394208, convergence 0.035754083810493054\n",
            "epoch: 25, loss: -1.404037422096788, convergence 0.01873806270257994\n",
            "epoch: 26, loss: -1.4078247939005712, convergence 0.0037873718037833015\n",
            "epoch: 27, loss: -1.3999930188716776, convergence 0.00783177502889365\n",
            "epoch: 28, loss: -1.3846648178020726, convergence 0.015328201069604974\n",
            "epoch: 29, loss: -1.3660868461380766, convergence 0.018577971663995996\n",
            "epoch: 30, loss: -1.3479737026110554, convergence 0.018113143527021203\n",
            "epoch: 31, loss: -1.3330778080784955, convergence 0.014895894532559906\n",
            "epoch: 32, loss: -1.323057305695224, convergence 0.010020502383271568\n",
            "epoch: 33, loss: -1.3185724280329656, convergence 0.0044848776622583575\n",
            "epoch: 34, loss: -1.31949272102539, convergence 0.0009202929924243541\n",
            "epoch: 35, loss: -1.3251237694107987, convergence 0.005631048385408732\n",
            "epoch: 36, loss: -1.3344154987044647, convergence 0.009291729293666062\n",
            "epoch: 37, loss: -1.3461371549540642, convergence 0.011721656249599421\n",
            "epoch: 38, loss: -1.359024977498791, convergence 0.012887822544726868\n",
            "epoch: 39, loss: -1.3719035541375546, convergence 0.012878576638763573\n",
            "epoch: 40, loss: -1.3837824528692848, convergence 0.011878898731730203\n",
            "epoch: 41, loss: -1.3939234498938866, convergence 0.010140997024601806\n",
            "epoch: 42, loss: -1.4018758546592143, convergence 0.007952404765327659\n",
            "epoch: 43, loss: -1.4074753527071806, convergence 0.005599498047966334\n",
            "epoch: 44, loss: -1.410811906696349, convergence 0.0033365539891683227\n",
            "epoch: 45, loss: -1.4121730389955336, convergence 0.0013611322991846642\n",
            "epoch: 46, loss: -1.4119756366882206, convergence 0.00019740230731302155\n",
            "epoch: 47, loss: -1.4106950895545718, convergence 0.0012805471336487795\n",
            "epoch: 48, loss: -1.4088031617317283, convergence 0.001891927822843531\n",
            "epoch: 49, loss: -1.4067217200949012, convergence 0.0020814416368271083\n",
            "epoch: 50, loss: -1.4047935083331287, convergence 0.0019282117617724914\n",
            "epoch: 51, loss: -1.4032663768801696, convergence 0.0015271314529590185\n",
            "epoch: 52, loss: -1.402293252172202, convergence 0.0009731247079676209\n",
            "epoch: 53, loss: -1.4019382091817407, convergence 0.00035504299046129617\n",
            "epoch: 54, loss: -1.4021909866578417, convergence 0.0002527774761009738\n",
            "epoch: 55, loss: -1.402981521283527, convergence 0.0007905346256853463\n",
            "epoch: 56, loss: -1.404198317682518, convergence 0.0012167963989908603\n",
            "epoch: 57, loss: -1.4057039663601727, convergence 0.0015056486776547917\n",
            "epoch: 58, loss: -1.4073521151799722, convergence 0.0016481488197994665\n",
            "epoch: 59, loss: -1.4090006166326834, convergence 0.0016485014527112263\n",
            "epoch: 60, loss: -1.4105249145746908, convergence 0.0015242979420073688\n",
            "epoch: 61, loss: -1.411825540618516, convergence 0.0013006260438253037\n",
            "epoch: 62, loss: -1.4128364158494406, convergence 0.0010108752309245794\n",
            "epoch: 63, loss: -1.4135266630538554, convergence 0.0006902472044147956\n",
            "epoch: 64, loss: -1.4139001192777083, convergence 0.0003734562238528749\n",
            "epoch: 65, loss: -1.4139912474372576, convergence 9.112815954925857e-05\n",
            "epoch: 66, loss: -1.4138579300828935, convergence 0.00013331735436405978\n",
            "epoch: 67, loss: -1.4135733347005028, convergence 0.0002845953823906733\n",
            "epoch: 68, loss: -1.4132148904726867, convergence 0.00035844422781616814\n",
            "epoch: 69, loss: -1.412856201159822, convergence 0.0003586893128646462\n",
            "epoch: 70, loss: -1.4125581221564119, convergence 0.0002980790034101677\n",
            "epoch: 71, loss: -1.4123633023962905, convergence 0.00019481976012136748\n",
            "epoch: 72, loss: -1.4122942310500801, convergence 6.907134621036981e-05\n",
            "epoch: 73, loss: -1.4123523876124535, convergence 5.81565623734015e-05\n",
            "epoch: 74, loss: -1.4125221732721287, convergence 0.00016978565967518122\n",
            "epoch: 75, loss: -1.412774933395386, convergence 0.0002527601232573673\n",
            "epoch: 76, loss: -1.4130744920184188, convergence 0.0002995586230327696\n",
            "epoch: 77, loss: -1.4133836830995565, convergence 0.0003091910811376941\n",
            "epoch: 78, loss: -1.413669154287452, convergence 0.00028547118789545856\n",
            "epoch: 79, loss: -1.413905143891462, convergence 0.0002359896040100562\n",
            "epoch: 80, loss: -1.4140758384050782, convergence 0.00017069451361617638\n",
            "epoch: 81, loss: -1.4141759600935855, convergence 0.00010012168850725978\n",
            "epoch: 82, loss: -1.4142104276665897, convergence 3.446757300418213e-05\n",
            "epoch: 83, loss: -1.4141907637732762, convergence 1.9663893313426328e-05\n",
            "epoch: 84, loss: -1.4141339799837473, convergence 5.6783789528891404e-05\n",
            "epoch: 85, loss: -1.414058471953998, convergence 7.550802974942705e-05\n",
            "epoch: 86, loss: -1.4139817717719874, convergence 7.670018201055839e-05\n",
            "epoch: 87, loss: -1.4139181936467176, convergence 6.357812526980311e-05\n",
            "epoch: 88, loss: -1.413877398217552, convergence 4.0795429165463304e-05\n",
            "epoch: 89, loss: -1.413863869495609, convergence 1.3528721943156796e-05\n",
            "epoch: 90, loss: -1.4138774672067378, convergence 1.3597711128854328e-05\n",
            "epoch: 91, loss: -1.4139135299119854, convergence 3.6062705247585924e-05\n",
            "epoch: 92, loss: -1.4139652024162905, convergence 5.16725043051558e-05\n",
            "epoch: 93, loss: -1.4140242890621404, convergence 5.9086645849859565e-05\n",
            "epoch: 94, loss: -1.4140825903591507, convergence 5.830129701034359e-05\n",
            "epoch: 95, loss: -1.4141337980155644, convergence 5.120765641364322e-05\n",
            "epoch: 96, loss: -1.4141731139598055, convergence 3.9315944241113954e-05\n",
            "epoch: 97, loss: -1.4141985951663252, convergence 2.5481206519684108e-05\n",
            "epoch: 98, loss: -1.4142099849179455, convergence 1.138975162029432e-05\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "nn_param_history = [nn_vars(tf.constant([1.]))]\n",
        "nn_cost_history = []\n",
        "\n",
        "for epoch in range(1,max_iterations+1):\n",
        "    with tf.GradientTape() as tape: #autodiff(backward)\n",
        "\n",
        "        loss = tf_cost_fn(*nn_vars(tf.constant([1.])))\n",
        "    if len(nn_cost_history) > 0:\n",
        "      conv = tf.abs(nn_cost_history[-1] - loss)\n",
        "    else:\n",
        "      conv = loss\n",
        "    nn_cost_history.append(loss)\n",
        "    gradients = tape.gradient(loss, nn_vars.trainable_variables);\n",
        "    optim.apply_gradients(zip(gradients, nn_vars.trainable_variables));\n",
        "    nn_param_history.append(nn_vars(tf.constant([1.])))\n",
        "    # conv = tf.abs(tf_cost_fn(*nn_vars(tf.constant([1.]))) - loss)\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "    print(\"epoch: {}, loss: {}, convergence {}\".format(epoch, loss, conv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "cWpZjJdaY9sh",
        "outputId": "1986b8e8-61cd-4203-fac0-e6d1c3a802bc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.style.use(\"seaborn\")\n",
        "plt.plot(gd_cost_history, \"b\", label=\"Gradient descent\")\n",
        "plt.plot(qngd_cost_history, \"g\", label=\"Quantum natural gradient descent\")\n",
        "plt.plot(nn_cost_history, \"r\", label=\"Neural network\")\n",
        "\n",
        "plt.ylabel(\"Cost function value\")\n",
        "plt.xlabel(\"Optimization steps\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V448pi2Hca3s"
      },
      "outputs": [],
      "source": [
        "#multi-variable은 구현이 어려움\n",
        "def pauli_string(sigma): #rotation operator 만들어줌\n",
        "    '''\n",
        "    translates sigma strings in Euler class to Pauli words in Pennylane.\n",
        "    '''\n",
        "    pauli_alphabet = [\"I\",\"X\",\"Y\",\"Z\"]\n",
        "    return \"\".join([pauli_alphabet[sigma[j]] for j in range(len(sigma))])\n",
        "def generate_qc_sk(eu):\n",
        "    '''\n",
        "    generates a universal parametric quantum circuit by using the KAK decomposition.\n",
        "    '''\n",
        "    pauli_ps = [pauli_string(s) for s in eu.basis]\n",
        "    dim = len(pauli_ps)\n",
        "\n",
        "    def qc_sk(theta):\n",
        "        for i in range(eu.dim):\n",
        "            qml.PauliRot(theta[i], pauli_ps[i], wires = list(range(eu.order))) #pauli rotation, pauli state\n",
        "    return qc_sk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-o6kdhg7eaOz"
      },
      "outputs": [],
      "source": [
        "def x_up(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.Hadamard(qw)) #|+> = |0> - H\n",
        "    else:\n",
        "        qml.Hadamard(wires=qw)\n",
        "\n",
        "def x_down(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.Hadamard(qw))\n",
        "        qml.adjoint(qml.PauliX(qw))\n",
        "    else:\n",
        "        qml.PauliX(wires=qw)\n",
        "        qml.Hadamard(wires=qw)\n",
        "\n",
        "def y_up(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.S(wires=qw))\n",
        "        qml.adjoint(qml.Hadamard(wires=qw))\n",
        "    else:\n",
        "        qml.Hadamard(wires=qw)\n",
        "        qml.S(wires=qw)\n",
        "\n",
        "def y_down(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.S(wires=qw))\n",
        "        qml.adjoint(qml.Hadamard(wires=qw))\n",
        "        qml.adjoint(qml.PauliX(wires=qw))\n",
        "    else:\n",
        "        qml.PauliX(wires=qw)\n",
        "        qml.Hadamard(wires=qw)\n",
        "        qml.S(wires=qw)\n",
        "\n",
        "def z_up(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.Identity(wires=qw))\n",
        "    else:\n",
        "        qml.Identity(wires=qw)\n",
        "\n",
        "def z_down(qw,adjoint=False):\n",
        "    if adjoint:\n",
        "        qml.adjoint(qml.PauliX(wires=qw))\n",
        "    else:\n",
        "        qml.PauliX(wires=qw)\n",
        "\n",
        "def initial_state(sigma,adjoint=False):\n",
        "    ###\n",
        "    0-5  # 0 xup 1 xdown 2yup ....\n",
        "    ###\n",
        "    init_state = [x_up,x_down,y_up,y_down,z_up,z_down]\n",
        "    order = len(sigma)\n",
        "    for i in range(0,order):\n",
        "        init_state[sigma[i]](i,adjoint)\n",
        "\n",
        "\n",
        "def sign_theta(eu):\n",
        "    sign_flat =  tf.constant(qml.pauli_decompose((2.j*sum([tf.reshape(Sigma(*[0]*eu.order).fundamental,[2**eu.order,2**eu.order])] + [tf.reshape(Sigma(*s).fundamental,[2**eu.order,2**eu.order]) for s in eu.basis])).numpy()).coeffs)\n",
        "    sign = tf.reshape(sign_flat,[4]*eu.order)\n",
        "    return tf.constant(np.array([sign[tuple(s)] for s in eu.basis]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f72M2ppgeDV3"
      },
      "outputs": [],
      "source": [
        "eu = Euler(2) # 2qubit : 2, single:1\n",
        "basis_ps = [pauli_string(s) for s in eu.basis]\n",
        "basis_sign = sign_theta(eu)\n",
        "qc_dev = qml.device('default.qubit', wires=eu.order)\n",
        "\n",
        "@qml.qnode(qc_dev, interface='tf')\n",
        "def qc_sk(theta,state,target):\n",
        "  initial_state(state)\n",
        "  for i in range(len(basis_ps)-1,-1,-1):\n",
        "    qml.PauliRot(basis_sign[i]*theta[i], basis_ps[i], wires = list(range(eu.order)))\n",
        "\n",
        "  qml.adjoint(target)\n",
        "  initial_state(state,adjoint=True) # VdaggerU(theta)|psi> = 1 -> 역으로 정의\n",
        "  return qml.probs(list(range(eu.order)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JqjjL5N2faR9"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "n_neuron = 8\n",
        "\n",
        "nn_theta = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input((1,)),\n",
        "    tf.keras.layers.Dense(n_neuron, activation='tanh'),\n",
        "    tf.keras.layers.Dense(16) # 2qubits -> 16, single -> 4\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "xIddEgEejWKK",
        "outputId": "d3ffa911-0e0e-4f5c-d257-8fe308596a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<Figure size 1584x216 with 1 Axes>, <Axes:>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1584x216 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "qml.draw_mpl(qc_sk)(*nn_theta(tf.constant([1.])),[0,4],qml.CNOT(wires=[0,1])) # input RZ RY RX RI H+ output measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Oe1XDWSxgsbJ",
        "outputId": "4fcfb091-9150-4313-df35-e30deaa5a0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 4.911426896763506, convergence 4.911426896763506\n",
            "epoch: 2, loss: 4.54373346050928, convergence 0.3676934362542257\n",
            "epoch: 3, loss: 4.248620189332189, convergence 0.2951132711770912\n",
            "epoch: 4, loss: 4.019242938360835, convergence 0.2293772509713543\n",
            "epoch: 5, loss: 3.833436847522165, convergence 0.1858060908386694\n",
            "epoch: 6, loss: 3.6739342502302184, convergence 0.15950259729194682\n",
            "epoch: 7, loss: 3.528907742237655, convergence 0.14502650799256323\n",
            "epoch: 8, loss: 3.390955050395375, convergence 0.1379526918422802\n",
            "epoch: 9, loss: 3.256995067436022, convergence 0.13395998295935296\n",
            "epoch: 10, loss: 3.1258683445886883, convergence 0.13112672284733362\n",
            "epoch: 11, loss: 2.995885801571095, convergence 0.12998254301759316\n",
            "epoch: 12, loss: 2.8644417381907945, convergence 0.13144406338030068\n",
            "epoch: 13, loss: 2.7291985917222967, convergence 0.13524314646849778\n",
            "epoch: 14, loss: 2.5885024066750066, convergence 0.1406961850472901\n",
            "epoch: 15, loss: 2.4411412592304504, convergence 0.1473611474445562\n",
            "epoch: 16, loss: 2.286332407045604, convergence 0.1548088521848463\n",
            "epoch: 17, loss: 2.1239698697384104, convergence 0.16236253730719374\n",
            "epoch: 18, loss: 1.9548383258576267, convergence 0.16913154388078366\n",
            "epoch: 19, loss: 1.7804235640932573, convergence 0.17441476176436943\n",
            "epoch: 20, loss: 1.602436967210414, convergence 0.17798659688284335\n",
            "epoch: 21, loss: 1.4222943446669447, convergence 0.18014262254346924\n",
            "epoch: 22, loss: 1.2406447799552938, convergence 0.18164956471165095\n",
            "epoch: 23, loss: 1.058104264995648, convergence 0.18254051495964574\n",
            "epoch: 24, loss: 0.8763640641216104, convergence 0.18174020087403764\n",
            "epoch: 25, loss: 0.6986624842839755, convergence 0.1777015798376349\n",
            "epoch: 26, loss: 0.5300462316195056, convergence 0.1686162526644699\n",
            "epoch: 27, loss: 0.37725900804184054, convergence 0.15278722357766505\n",
            "epoch: 28, loss: 0.24750966556119303, convergence 0.1297493424806475\n",
            "epoch: 29, loss: 0.14688984060974297, convergence 0.10061982495145005\n",
            "epoch: 30, loss: 0.0788975942386303, convergence 0.06799224637111267\n",
            "epoch: 31, loss: 0.04279582007321148, convergence 0.03610177416541882\n",
            "epoch: 32, loss: 0.03295347260616055, convergence 0.009842347467050927\n",
            "epoch: 33, loss: 0.04062930227565642, convergence 0.0076758296694958705\n",
            "epoch: 34, loss: 0.05720786752180429, convergence 0.01657856524614787\n",
            "epoch: 35, loss: 0.07617492686999427, convergence 0.018967059348189985\n",
            "epoch: 36, loss: 0.09335175051689304, convergence 0.017176823646898765\n",
            "epoch: 37, loss: 0.10646320840594514, convergence 0.013111457889052103\n",
            "epoch: 38, loss: 0.11458545801860343, convergence 0.008122249612658283\n",
            "epoch: 39, loss: 0.11756555144071978, convergence 0.002980093422116359\n",
            "epoch: 40, loss: 0.11560908621788402, convergence 0.001956465222835768\n",
            "epoch: 41, loss: 0.10919749367960097, convergence 0.006411592538283051\n",
            "epoch: 42, loss: 0.09907569859865861, convergence 0.010121795080942353\n",
            "epoch: 43, loss: 0.08621534425294486, convergence 0.012860354345713754\n",
            "epoch: 44, loss: 0.07181899502752997, convergence 0.014396349225414884\n",
            "epoch: 45, loss: 0.05735768696928456, convergence 0.014461308058245415\n",
            "epoch: 46, loss: 0.04442761331467082, convergence 0.012930073654613738\n",
            "epoch: 47, loss: 0.03434630907701053, convergence 0.010081304237660293\n",
            "epoch: 48, loss: 0.02776531402030402, convergence 0.006580995056706507\n",
            "epoch: 49, loss: 0.024595760522188703, convergence 0.0031695534981153184\n",
            "epoch: 50, loss: 0.02412747173338603, convergence 0.0004682887888026732\n",
            "epoch: 51, loss: 0.025181670437662063, convergence 0.0010541987042760326\n",
            "epoch: 52, loss: 0.026446827629541425, convergence 0.0012651571918793625\n",
            "epoch: 53, loss: 0.026951594484712738, convergence 0.0005047668551713125\n",
            "epoch: 54, loss: 0.026268741404057327, convergence 0.0006828530806554101\n",
            "epoch: 55, loss: 0.024384483961798042, convergence 0.001884257442259285\n",
            "epoch: 56, loss: 0.021559789601991786, convergence 0.002824694359806257\n",
            "epoch: 57, loss: 0.01823431135191622, convergence 0.003325478250075564\n",
            "epoch: 58, loss: 0.014829081092365803, convergence 0.003405230259550418\n",
            "epoch: 59, loss: 0.011613054390254396, convergence 0.0032160267021114075\n",
            "epoch: 60, loss: 0.008787842308127236, convergence 0.00282521208212716\n",
            "epoch: 61, loss: 0.006561145460243112, convergence 0.0022266968478841243\n",
            "epoch: 62, loss: 0.005063487728403593, convergence 0.001497657731839519\n",
            "epoch: 63, loss: 0.004303023445135112, convergence 0.0007604642832684805\n",
            "epoch: 64, loss: 0.0042157989532704, convergence 8.722449186471248e-05\n",
            "epoch: 65, loss: 0.0046535083854516435, convergence 0.0004377094321812436\n",
            "epoch: 66, loss: 0.0053565727825469756, convergence 0.0007030643970953321\n",
            "epoch: 67, loss: 0.006047920391617723, convergence 0.0006913476090707471\n",
            "epoch: 68, loss: 0.006529695715073558, convergence 0.0004817753234558353\n",
            "epoch: 69, loss: 0.006678493770607474, convergence 0.00014879805553391634\n",
            "epoch: 70, loss: 0.006451153831139367, convergence 0.0002273399394681075\n",
            "epoch: 71, loss: 0.005917424762614143, convergence 0.0005337290685252238\n",
            "epoch: 72, loss: 0.00520724383886928, convergence 0.0007101809237448631\n",
            "epoch: 73, loss: 0.0044295633560075265, convergence 0.0007776804828617534\n",
            "epoch: 74, loss: 0.0036621529884485193, convergence 0.0007674103675590072\n",
            "epoch: 75, loss: 0.0029511016307526283, convergence 0.000711051357695891\n",
            "epoch: 76, loss: 0.0022967944272103136, convergence 0.0006543072035423148\n",
            "epoch: 77, loss: 0.00169087034279225, convergence 0.0006059240844180636\n",
            "epoch: 78, loss: 0.0011623432130578593, convergence 0.0005285271297343908\n",
            "epoch: 79, loss: 0.0007628078426098472, convergence 0.000399535370448012\n",
            "epoch: 80, loss: 0.0005345505429916741, convergence 0.0002282572996181731\n",
            "epoch: 81, loss: 0.000495879244434505, convergence 3.867129855716911e-05\n",
            "epoch: 82, loss: 0.0006199662123589356, convergence 0.00012408696792443052\n",
            "epoch: 83, loss: 0.0008306133207029998, convergence 0.0002106471083440642\n",
            "epoch: 84, loss: 0.0010380980779574056, convergence 0.00020748475725440585\n",
            "epoch: 85, loss: 0.0011741990873120889, convergence 0.00013610100935468328\n",
            "epoch: 86, loss: 0.0012045294681377605, convergence 3.0330380825671632e-05\n",
            "epoch: 87, loss: 0.0011336718870127438, convergence 7.085758112501672e-05\n",
            "epoch: 88, loss: 0.000997395796691003, convergence 0.00013627609032174082\n",
            "epoch: 89, loss: 0.0008388240338785824, convergence 0.00015857176281242058\n",
            "epoch: 90, loss: 0.0006906726690225629, convergence 0.00014815136485601954\n",
            "epoch: 91, loss: 0.0005697752506391707, convergence 0.00012089741838339219\n",
            "epoch: 92, loss: 0.00047668548210044737, convergence 9.30897685387233e-05\n",
            "epoch: 93, loss: 0.0004020830515777396, convergence 7.460243052270776e-05\n",
            "epoch: 94, loss: 0.0003381455572276648, convergence 6.393749435007479e-05\n",
            "epoch: 95, loss: 0.00028483484406338633, convergence 5.331071316427849e-05\n",
            "epoch: 96, loss: 0.00024747809094038953, convergence 3.73567531229968e-05\n",
            "epoch: 97, loss: 0.0002305697089654002, convergence 1.690838197498934e-05\n",
            "epoch: 98, loss: 0.00023249529662039414, convergence 1.9255876549939543e-06\n",
            "epoch: 99, loss: 0.0002435228054987748, convergence 1.1027508878380665e-05\n",
            "epoch: 100, loss: 0.00024936361598426604, convergence 5.840810485491232e-06\n",
            "epoch: 101, loss: 0.00023936433909343968, convergence 9.999276890826359e-06\n",
            "epoch: 102, loss: 0.00021162218179648917, convergence 2.774215729695051e-05\n",
            "epoch: 103, loss: 0.00017178583615828913, convergence 3.9836345638200044e-05\n",
            "epoch: 104, loss: 0.00012984921201508737, convergence 4.193662414320176e-05\n",
            "epoch: 105, loss: 9.55699980268454e-05, convergence 3.427921398824196e-05\n",
            "epoch: 106, loss: 7.391436848269706e-05, convergence 2.1655629544148347e-05\n",
            "epoch: 107, loss: 6.516809642787003e-05, convergence 8.746272054827031e-06\n",
            "epoch: 108, loss: 6.691564124861049e-05, convergence 1.7475448207404654e-06\n",
            "epoch: 109, loss: 7.467295966112619e-05, convergence 7.757318412515701e-06\n",
            "epoch: 110, loss: 8.385224773566069e-05, convergence 9.179288074534497e-06\n",
            "epoch: 111, loss: 9.126330139341832e-05, convergence 7.411053657757627e-06\n",
            "epoch: 112, loss: 9.44305657982758e-05, convergence 3.167264404857484e-06\n",
            "epoch: 113, loss: 9.200467560976922e-05, convergence 2.4258901885065853e-06\n",
            "epoch: 114, loss: 8.416778605868647e-05, convergence 7.836889551082749e-06\n",
            "epoch: 115, loss: 7.167272203190134e-05, convergence 1.249506402678513e-05\n",
            "epoch: 116, loss: 5.6160119486836635e-05, convergence 1.5512602545064702e-05\n",
            "epoch: 117, loss: 4.036138065399708e-05, convergence 1.5798738832839554e-05\n",
            "epoch: 118, loss: 2.6832304166668486e-05, convergence 1.3529076487328595e-05\n",
            "epoch: 119, loss: 1.7425961574835114e-05, convergence 9.406342591833372e-06\n",
            "epoch: 120, loss: 1.2875632597997289e-05, convergence 4.550328976837825e-06\n",
            "epoch: 121, loss: 1.224865223703464e-05, convergence 6.269803609626479e-07\n",
            "epoch: 122, loss: 1.3871445303847985e-05, convergence 1.622793066813344e-06\n",
            "epoch: 123, loss: 1.631340015306737e-05, convergence 2.4419548492193854e-06\n",
            "epoch: 124, loss: 1.8554923545410773e-05, convergence 2.241523392343403e-06\n",
            "epoch: 125, loss: 2.0207007714989622e-05, convergence 1.652084169578849e-06\n",
            "epoch: 126, loss: 2.1196092862552085e-05, convergence 9.89085147562463e-07\n",
            "epoch: 127, loss: 2.125106227190976e-05, convergence 5.496940935767469e-08\n",
            "epoch: 128, loss: 2.013158607583332e-05, convergence 1.119476196076441e-06\n",
            "epoch: 129, loss: 1.781711180581791e-05, convergence 2.3144742700154097e-06\n",
            "epoch: 130, loss: 1.4569525649710258e-05, convergence 3.2475861561076513e-06\n",
            "epoch: 131, loss: 1.1042453707199584e-05, convergence 3.5270719425106734e-06\n",
            "epoch: 132, loss: 8.006745197741694e-06, convergence 3.0357085094578906e-06\n",
            "epoch: 133, loss: 5.981213982098055e-06, convergence 2.025531215643639e-06\n",
            "epoch: 134, loss: 5.102158200709361e-06, convergence 8.790557813886934e-07\n",
            "epoch: 135, loss: 5.079713445987899e-06, convergence 2.2444754721462346e-08\n",
            "epoch: 136, loss: 5.365761194919827e-06, convergence 2.8604774893192797e-07\n",
            "epoch: 137, loss: 5.488550901211475e-06, convergence 1.2278970629164832e-07\n",
            "epoch: 138, loss: 5.242691279105927e-06, convergence 2.458596221055487e-07\n",
            "epoch: 139, loss: 4.704143562794805e-06, convergence 5.385477163111219e-07\n",
            "epoch: 140, loss: 4.0983120402904305e-06, convergence 6.058315225043742e-07\n",
            "epoch: 141, loss: 3.6018808461379948e-06, convergence 4.964311941524358e-07\n",
            "epoch: 142, loss: 3.2327943177978824e-06, convergence 3.6908652834011235e-07\n",
            "epoch: 143, loss: 2.916219242643514e-06, convergence 3.1657507515436834e-07\n",
            "epoch: 144, loss: 2.5781748947961702e-06, convergence 3.3804434784734383e-07\n",
            "epoch: 145, loss: 2.2127063866728136e-06, convergence 3.6546850812335663e-07\n",
            "epoch: 146, loss: 1.8932638357060938e-06, convergence 3.194425509667198e-07\n",
            "epoch: 147, loss: 1.6976219114894775e-06, convergence 1.956419242166163e-07\n",
            "epoch: 148, loss: 1.6436709575629038e-06, convergence 5.3950953926573675e-08\n",
            "epoch: 149, loss: 1.6908099756962613e-06, convergence 4.7139018133357524e-08\n",
            "epoch: 150, loss: 1.7476882392397641e-06, convergence 5.687826354350278e-08\n",
            "epoch: 151, loss: 1.7225763375705583e-06, convergence 2.511190166920585e-08\n",
            "epoch: 152, loss: 1.5806591147082827e-06, convergence 1.419172228622756e-07\n",
            "epoch: 153, loss: 1.336249692229785e-06, convergence 2.444094224784976e-07\n",
            "epoch: 154, loss: 1.044975553310401e-06, convergence 2.9127413891938403e-07\n",
            "epoch: 155, loss: 7.70992838905471e-07, convergence 2.7398271440493005e-07\n",
            "epoch: 156, loss: 5.480719489359132e-07, convergence 2.2292088996955783e-07\n",
            "epoch: 157, loss: 3.8960869253035213e-07, convergence 1.5846325640556103e-07\n",
            "epoch: 158, loss: 2.9690451919250904e-07, convergence 9.270417333784309e-08\n",
            "epoch: 159, loss: 2.646554413399116e-07, convergence 3.224907785259745e-08\n",
            "epoch: 160, loss: 2.8960501463615174e-07, convergence 2.4949573296240146e-08\n",
            "epoch: 161, loss: 3.5176557711125156e-07, convergence 6.216056247509982e-08\n",
            "epoch: 162, loss: 4.1969735309344003e-07, convergence 6.793177598218847e-08\n",
            "epoch: 163, loss: 4.6519232754782536e-07, convergence 4.549497445438533e-08\n",
            "epoch: 164, loss: 4.681744572021529e-07, convergence 2.9821296543275366e-09\n",
            "epoch: 165, loss: 4.304689523726424e-07, convergence 3.7705504829510517e-08\n",
            "epoch: 166, loss: 3.6581945850322484e-07, convergence 6.464949386941754e-08\n",
            "epoch: 167, loss: 2.888084033036975e-07, convergence 7.701105519952733e-08\n",
            "epoch: 168, loss: 2.1441235886765497e-07, convergence 7.439604443604253e-08\n",
            "epoch: 169, loss: 1.501049728958037e-07, convergence 6.430738597185126e-08\n",
            "epoch: 170, loss: 1.0069357969655357e-07, convergence 4.941139319925014e-08\n",
            "epoch: 171, loss: 6.968013677166596e-08, convergence 3.101344292488761e-08\n",
            "epoch: 172, loss: 5.769067967786867e-08, convergence 1.1989457093797284e-08\n",
            "epoch: 173, loss: 6.314360301296063e-08, convergence 5.452923335091953e-09\n",
            "epoch: 174, loss: 7.985885852601626e-08, convergence 1.6715255513055638e-08\n",
            "epoch: 175, loss: 9.74276433973742e-08, convergence 1.7568784871357934e-08\n",
            "epoch: 176, loss: 1.0536651084969151e-07, convergence 7.938867452317311e-09\n",
            "epoch: 177, loss: 9.862746186417581e-08, convergence 6.739048985515694e-09\n",
            "epoch: 178, loss: 7.984432104368722e-08, convergence 1.8783140820488597e-08\n",
            "epoch: 179, loss: 5.778636558151362e-08, convergence 2.2057955462173595e-08\n",
            "epoch: 180, loss: 4.075387005642028e-08, convergence 1.703249552509334e-08\n",
            "epoch: 181, loss: 3.239972590218798e-08, convergence 8.354144154232301e-09\n",
            "epoch: 182, loss: 3.1221454421803685e-08, convergence 1.1782714803842964e-09\n",
            "epoch: 183, loss: 3.298489203995558e-08, convergence 1.7634376181518974e-09\n",
            "epoch: 184, loss: 3.4174088336946795e-08, convergence 1.1891962969912129e-09\n",
            "epoch: 185, loss: 3.3570882407296665e-08, convergence 6.032059296501302e-10\n",
            "epoch: 186, loss: 3.1481182327652846e-08, convergence 2.0897000796438192e-09\n",
            "epoch: 187, loss: 2.8749152169993408e-08, convergence 2.7320301576594375e-09\n",
            "epoch: 188, loss: 2.5435250905481155e-08, convergence 3.3139012645122534e-09\n",
            "epoch: 189, loss: 2.1171989272872338e-08, convergence 4.263261632608817e-09\n",
            "epoch: 190, loss: 1.586686482202282e-08, convergence 5.305124450849519e-09\n",
            "epoch: 191, loss: 1.0119241000694501e-08, convergence 5.747623821328318e-09\n",
            "epoch: 192, loss: 5.504516287224703e-09, convergence 4.614724713469798e-09\n",
            "epoch: 193, loss: 3.2635935065528088e-09, convergence 2.2409227806718945e-09\n",
            "epoch: 194, loss: 3.686149829817964e-09, convergence 4.2255632326515524e-10\n",
            "epoch: 195, loss: 5.923524670770064e-09, convergence 2.2373748409521e-09\n",
            "epoch: 196, loss: 8.451899957684361e-09, convergence 2.5283752869142972e-09\n",
            "epoch: 197, loss: 1.0238268122186867e-08, convergence 1.7863681645025054e-09\n",
            "epoch: 198, loss: 1.0817633122606196e-08, convergence 5.793650004193296e-10\n",
            "epoch: 199, loss: 1.0260192029321047e-08, convergence 5.574410932851492e-10\n",
            "epoch: 200, loss: 8.908516924144294e-09, convergence 1.3516751051767528e-09\n",
            "epoch: 201, loss: 7.1086107000795096e-09, convergence 1.7999062240647845e-09\n",
            "epoch: 202, loss: 5.172022365940165e-09, convergence 1.936588334139344e-09\n",
            "epoch: 203, loss: 3.360037137412064e-09, convergence 1.8119852285281013e-09\n",
            "epoch: 204, loss: 1.998899601396431e-09, convergence 1.361137536015633e-09\n",
            "epoch: 205, loss: 1.2784880931704379e-09, convergence 7.204115082259932e-10\n",
            "epoch: 206, loss: 1.143034222828021e-09, convergence 1.3545387034241685e-10\n",
            "epoch: 207, loss: 1.3961242162352505e-09, convergence 2.530899934072295e-10\n",
            "epoch: 208, loss: 1.7911468974673994e-09, convergence 3.950226812321489e-10\n",
            "epoch: 209, loss: 2.125637110772516e-09, convergence 3.344902133051164e-10\n",
            "epoch: 210, loss: 2.324148651311475e-09, convergence 1.9851154053895925e-10\n"
          ]
        }
      ],
      "source": [
        "max_iterations = 300\n",
        "conv_tol = 1e-10\n",
        "# step_size = 0.1\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "#optim = qml.QNGOptimizer(stepsize=0.1, approx=\"block-diag\")\n",
        "\n",
        "nn_param_history = [nn_theta(tf.constant([1.]))]\n",
        "nn_cost_history = []\n",
        "\n",
        "for epoch in range(1,max_iterations+1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_1 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[0,0],qml.CNOT(wires=[0,1]))[0]; # minimal basis 몇개?  6개/9개 (O)\n",
        "        loss_2 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[0,4],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_3 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[0,5],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_4 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[4,0],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_5 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[4,4],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_6 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[4,5],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_7 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[5,0],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_8 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[5,4],qml.CNOT(wires=[0,1]))[0];\n",
        "        loss_9 = 1.0 - qc_sk(*nn_theta(tf.constant([1.])),[5,5],qml.CNOT(wires=[0,1]))[0];\n",
        "\n",
        "\n",
        "        loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6 + loss_7 + loss_8 + loss_9\n",
        "    if len(nn_cost_history) > 0:\n",
        "      conv = tf.abs(nn_cost_history[-1] - loss)\n",
        "    else:\n",
        "      conv = loss\n",
        "    nn_cost_history.append(loss)\n",
        "    gradients = tape.gradient(loss, nn_theta.trainable_variables);\n",
        "    optim.apply_gradients(zip(gradients, nn_theta.trainable_variables));\n",
        "    nn_param_history.append(nn_theta(tf.constant([1.])))\n",
        "    # conv = tf.abs(tf_cost_fn(*nn_theta(tf.constant([1.]))) - loss)\n",
        "    if conv <= conv_tol:\n",
        "        break\n",
        "    print(\"epoch: {}, loss: {}, convergence {}\".format(epoch, loss, conv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "293KZRPCmTvR",
        "outputId": "7929f509-381f-457e-8245-961d3b58a2f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=complex128, numpy=\n",
              "array([[ 0.40777591+0.28934575j,  0.28937347-0.40775075j,\n",
              "         0.40776867+0.28934859j, -0.28937259+0.40775701j],\n",
              "       [ 0.2893642 -0.40775979j,  0.40775815+0.28937168j,\n",
              "        -0.2893602 +0.40776516j,  0.40776257+0.28935372j],\n",
              "       [ 0.4077638 +0.28935871j, -0.289383  +0.40774421j,\n",
              "         0.4077713 +0.28934883j,  0.28937637-0.40775422j],\n",
              "       [-0.2893619 +0.40775983j,  0.407762  +0.2893647j ,\n",
              "         0.28936333-0.40776101j,  0.40775929+0.28936484j]])>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# qc_sk(*nn_theta(tf.constant([1.])),[5],qml.Hadamard(wires=[0]))\n",
        "qc_sk_mat = qml.matrix(qc_sk)(*nn_theta(tf.constant([1.])),[1],qml.S(wires=[0]));qc_sk_mat #qml matrix : 위의 circuit을 matrix form으로 바꿈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Nfw1J7uuhqiB"
      },
      "outputs": [],
      "source": [
        "def generate_second_kind(eu):\n",
        "  eu_fund = tf.constant(np.array([tf.reshape(Sigma(*s).fundamental,[2**eu.order,2**eu.order]) for s in eu.basis]),dtype=tf.complex128);\n",
        "  dim = len(eu.basis)\n",
        "  identity = tf.eye(2**eu.order,dtype=tf.complex128)\n",
        "  @tf.function\n",
        "  def second_kind(theta):\n",
        "    c = tf.cast(tf.cos(theta/2),tf.complex128)\n",
        "    s = tf.cast(2*tf.sin(theta/2),tf.complex128)\n",
        "    ret = c[0] * identity + s[0] * eu_fund[0]\n",
        "    for i in range(1,dim):\n",
        "      ret = tf.matmul(ret,(c[i] * identity + s[i] * eu_fund[i]))\n",
        "    return ret\n",
        "  return second_kind\n",
        "\n",
        "fn_sk = generate_second_kind(eu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDcy6OVpidwT",
        "outputId": "73dc09e5-be0c-4bb9-d8b6-0457ff2988d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=complex128, numpy=\n",
              "array([[ 0.49823322+0.04200853j, -0.04199056+0.49823435j,\n",
              "         0.49823155+0.04200235j,  0.04199701-0.4982324j ],\n",
              "       [-0.04199632+0.49823386j,  0.49823413+0.04199776j,\n",
              "         0.04198963-0.49823302j,  0.49823145+0.04200356j],\n",
              "       [ 0.04201909-0.49823012j,  0.49823043+0.0420205j ,\n",
              "        -0.04201326+0.49823284j,  0.49823124+0.04202721j],\n",
              "       [ 0.49822995+0.04202622j,  0.04200823-0.49823104j,\n",
              "         0.49823187+0.04201975j, -0.04201443+0.49823274j]])>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.matmul(fn_sk(*(nn_theta(tf.constant([1.])))),tf.linalg.adjoint(qc_sk_mat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzAXYgD6tDKy",
        "outputId": "4ea82241-d534-47e3-bec3-1128b173fdc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=complex128, numpy=\n",
              "array([[ 7.64035746e-01+6.45173819e-01j, -2.72243385e-06-2.10572145e-06j,\n",
              "         4.26830403e-07-3.49457472e-06j,  6.24034408e-06+5.20982421e-06j],\n",
              "       [ 2.53190011e-06+2.33141693e-06j,  7.64014723e-01+6.45198715e-01j,\n",
              "         2.42709394e-07+2.07910897e-06j,  1.60401818e-06-8.80766702e-06j],\n",
              "       [-6.18108571e-06-5.27993082e-06j,  8.41485164e-06-3.05582991e-06j,\n",
              "         5.71206373e-06+7.90091154e-06j,  7.63994701e-01+6.45222423e-01j],\n",
              "       [ 3.37380132e-06-1.00601176e-06j, -2.09047978e-06+1.08896516e-07j,\n",
              "         7.64016828e-01+6.45196223e-01j, -8.74562683e-06-4.30875458e-06j]])>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fn_sk(*(nn_theta(tf.constant([1.]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xI5jQtUfpVcX"
      },
      "outputs": [],
      "source": [
        "h_gate = tf.cast(tf.constant([[1.,1.],[1.,-1.]])/tf.sqrt(2.),dtype=tf.complex128);\n",
        "s_gate = tf.constant([[1,0],[0,-1.0j]],dtype=tf.complex128);\n",
        "t_gate = tf.constant(np.array([[1,0],[0,np.exp(1.0j*np.pi/8)]],dtype=np.complex128));\n",
        "cnot_gate = tf.constant(np.array([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=np.complex128));"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
