{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lichkim/VQE_Summer_Internship/blob/master/tutorial_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzZ7a8p-_cpa",
        "outputId": "b976f090-2479-4ea0-de30-c97d238a3809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nlopt in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from nlopt) (1.22.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-01 07:52:18.474441: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-01 07:52:18.517883: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-08-01 07:52:18.518887: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-01 07:52:19.480545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# install required packages\n",
        "!pip install -U nlopt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nlopt\n",
        "from itertools import product\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MIMdsz_u_j04"
      },
      "outputs": [],
      "source": [
        "class Sigma:\n",
        "\n",
        "    '''\n",
        "    Sigma class generates algebraic primitives required to compute the system of implicit ODEs for Euler angles.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, *args):\n",
        "            self.string = [i for i in args] #유저가 입력하는 args를 리스트로 바꿉니다 예시에는 args로 1, 2가 들어갔습니다. 첫 번째 args는 사용할 pauli matrix를 결정합니다.(1:I, 2:X, 3:Y, 4:Z)\n",
        "            self.order = len(args)          #그 string의 길이를 order로 합니다 지금은 1, 2이므로 2입니다.\n",
        "            self.digit = sum([args[i]*4**(self.order-i-1) for i in range(len(args))])   #(4 * args[i])^(args길이-1-i)를 i=0~args길이만큼 반복한 것을 더합니다. 예시의 경우 4+1=5 입니다.\n",
        "            \n",
        "            #single qubit에 가하는 pauli gate의 matrix form입니다.\n",
        "            self.pauli = tf.constant(   #4개의 pauli matrix입니다. 순서대로 I, X, Y, Z입니다.\n",
        "                [  \n",
        "                    [\n",
        "                        [-1j/2,0],\n",
        "                        [0,-1j/2]\n",
        "                    ],\n",
        "                    [\n",
        "                        [0, 1j/2],\n",
        "                        [1j/2, 0]\n",
        "                    ],\n",
        "                    [\n",
        "                        [0, -1/2],\n",
        "                        [1/2, 0]\n",
        "                    ],\n",
        "                    [\n",
        "                        [1j/2, 0],\n",
        "                        [0, -1j/2]\n",
        "                    ]\n",
        "                ],dtype=tf.complex64,shape=(4,2,2))\n",
        "            \n",
        "            \n",
        "            self.__struct_const_0 = tf.constant(   \n",
        "                [\n",
        "                    np.zeros([4,4]),\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, -1.0],[0.0, 0.0, 1.0, 0.0]],\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 1.0],[0.0, 0.0, 0.0, 0.0],[0.0, -1.0, 0.0, 0.0]],\n",
        "                    [[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, -1.0, 0.0],[0.0, 1.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]\n",
        "                    ], dtype=tf.complex64\n",
        "            )\n",
        "            self.__struct_const_1 = tf.constant(\n",
        "                [\n",
        "                    -1j*np.eye(4),\n",
        "                    -1j*np.array([[0.0, 1.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]),\n",
        "                    -1j*np.array([[0.0, 0.0, 1.0, 0.0],[0.0, 0.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0]]),\n",
        "                    -1j*np.array([[0.0, 0.0, 0.0, 1.0],[0.0, 0.0, 0.0, 0.0],[0.0, 0.0, 0.0, 0.0],[1.0, 0.0, 0.0, 0.0]])\n",
        "                    ], dtype=tf.complex64\n",
        "            )\n",
        "            self.__vector = tf.constant([\n",
        "                [1,0,0,0],\n",
        "                [0,1,0,0],\n",
        "                [0,0,1,0],\n",
        "                [0,0,0,1]], dtype=tf.float32\n",
        "            )\n",
        "            \n",
        "            self.index_perm = list(range(0,self.order*2,2)) + list(range(1,self.order*2+1,2))   #index permuation. 0부터의 짝수 이후 1부터의 홀수를 셉니다\n",
        "            self.fundamental = self.rep_fundamental()   #추후 rep_fundamental 메서드에 설명합니다. 파이썬 문법적으로는 rep_fundamental메서드가 객체가 되어 이를 부른것입니다.\n",
        "            self.adjoint = self.rep_adjoint()           #추후 rep_adjoint 메서드에 설명합니다.\n",
        "            \n",
        "            #self.fundamental_z를 구하기 위해 필요한 부가적인 연산들입니다.\n",
        "            self.fundamental_z = tf.eye(2,dtype=tf.complex64)\n",
        "            for i in range(1,self.order):\n",
        "                self.fundamental_z = tf.tensordot(self.fundamental_z,tf.eye(2,dtype=tf.complex64),axes=0)\n",
        "            self.fundamental_z = tf.transpose(self.fundamental_z,perm=self.index_perm)\n",
        "            \n",
        "            #self.adjoint_z를 구하기 위해 필요한 부가적인 연산들입니다.\n",
        "            self.adjoint_z = tf.eye(4,dtype=tf.float32)\n",
        "            for i in range(1,self.order):\n",
        "                self.adjoint_z = tf.tensordot(self.adjoint_z,tf.eye(4,dtype=tf.float32),axes=0)\n",
        "            self.adjoint_z = tf.transpose(self.adjoint_z,perm=self.index_perm)\n",
        "            self.adjoint_square = tf.tensordot(self.adjoint,self.adjoint,axes=self.order)\n",
        "            self.adj_vec = self.adj_vector()            #추후 adj_vector 메서드에 설명합니다.\n",
        "\n",
        "    def rep_fundamental(self):\n",
        "        index = self.string\n",
        "        ret = self.pauli[index[0]]  # args의 첫 번째 숫자에 따라 다른 pauli matrix를 부릅니다.\n",
        "        for i in range(1,len(index)):\n",
        "            ret = 2j*tf.tensordot(ret,self.pauli[index[i]],axes=0)\n",
        "        return tf.transpose(ret,perm=self.index_perm)\n",
        "\n",
        "    def rep_adjoint(self):\n",
        "        return tf.transpose(tf.cast(self.__rep_adjoint(self.string),dtype=tf.float32),perm=self.index_perm)\n",
        "\n",
        "    def __rep_adjoint(self,sigma,center=False,grade=False):\n",
        "        order = len(sigma)  #sigma는 order의 값을 구하는대만 사용됩니다. 이럴거면 코드 자체를 sigma를 받는게 아니라 order를 받는걸로 짜면 안되나\n",
        "        if grade:\n",
        "            ret = self.__struct_const_1[sigma[0]]\n",
        "        else:\n",
        "            ret = self.__struct_const_0[sigma[0]]\n",
        "\n",
        "        \"\"\"\n",
        "        Base case: order == 1 \n",
        "            -grade == False  // ret=__struct_const_0(sigma[0])\n",
        "            -grade == True   // ret=__struct_const_1(sigma[0])\n",
        "            \n",
        "        이후의 케이스(order > 1)에 대해서는 함수를 재귀적으로 호출합니다.\n",
        "        \"\"\"\n",
        "        if order > 1:\n",
        "            if grade:\n",
        "                ret = 1.j* (\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=True),  self.__rep_adjoint([sigma[order-1]],center=True,grade=True),axes=0) +\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=False), self.__rep_adjoint([sigma[order-1]],center=True,grade=False),axes=0)\n",
        "                    )\n",
        "            else:\n",
        "                ret = 1.j* (\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=True),  self.__rep_adjoint([sigma[order-1]],center=True,grade=False),axes=0) +\n",
        "                    tf.tensordot(self.__rep_adjoint(sigma[0:order-1],center=True,grade=False), self.__rep_adjoint([sigma[order-1]],center=True,grade=True),axes=0)\n",
        "                    )\n",
        "\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def adj_vector(self):\n",
        "        index = self.string\n",
        "        ret = self.__vector[index[0]]\n",
        "        for i in range(1,len(index)):\n",
        "            ret = tf.tensordot(ret,self.__vector[index[i]],axes=0)\n",
        "        return ret\n",
        "    \n",
        "    @tf.function\n",
        "    def exp_fundamental(self,x):\n",
        "        s = tf.cast(tf.math.sin(x/2),tf.complex64)\n",
        "        c = tf.cast(tf.math.cos(x/2),tf.complex64)\n",
        "        return tf.tensordot(c, self.fundamental_z,axes=0) + tf.tensordot(2*s,  self.fundamental,axes=0)\n",
        "    \n",
        "    @tf.function\n",
        "    def exp_adjoint(self,x):\n",
        "        return self.adjoint_z + tf.tensordot(tf.math.sin(x), self.adjoint,axes=0) + tf.tensordot((1 - tf.math.cos(x)), self.adjoint_square,axes=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgilQqWLhTF5",
        "outputId": "a089c0f9-4db2-4061-eed3-3fcb5ba0cda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-01 07:52:20.970575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-01 07:52:20.971279: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2, 2, 2), dtype=complex64, numpy=\n",
              "array([[[[ 0. +0.j,  0. +0.j],\n",
              "         [ 0. +0.j,  0.5+0.j]],\n",
              "\n",
              "        [[ 0. +0.j,  0. +0.j],\n",
              "         [-0.5+0.j,  0. +0.j]]],\n",
              "\n",
              "\n",
              "       [[[ 0. +0.j,  0.5+0.j],\n",
              "         [ 0. +0.j,  0. +0.j]],\n",
              "\n",
              "        [[-0.5+0.j,  0. +0.j],\n",
              "         [ 0. +0.j,  0. +0.j]]]], dtype=complex64)>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s1 = Sigma(1,2)\n",
        "s1.fundamental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j4w10LAq_2vU"
      },
      "outputs": [],
      "source": [
        "class Euler():\n",
        "    '''\n",
        "    Euler class genetates various list of indices of sigma strings of the rank n.\n",
        "    '''\n",
        "    def __init__(self, order):\n",
        "        self.order = order\n",
        "        self.basis = np.array(list(product([0,1,2,3], repeat = order)))\n",
        "        self.toral = np.array(list(product([0,3], repeat = order)))\n",
        "        self.kprime = self.__kprime(order)\n",
        "        self.aprime = self.__aprime(order)\n",
        "        self.k = self.__k(order)\n",
        "        self.a = self.__a(order)\n",
        "        self.kak = np.concatenate((self.k,self.a,self.k),axis=0)\n",
        "        self.generator = self.__generator(order)\n",
        "        self.dim = len(self.kak)\n",
        "\n",
        "    def __kak(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[3],[1],[3]])\n",
        "        else:\n",
        "            # return self.__a(order-1)\n",
        "            return np.concatenate((self.__k(order),self.__a(order),self.__k(order)),axis=0)\n",
        "\n",
        "    def __kprime(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([],dtype=np.int32)\n",
        "        else:\n",
        "            # print(self.__kak(order-1))\n",
        "            return np.insert(self.__kak(order-1),0,0,axis=1)\n",
        "\n",
        "    def __aprime(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([],dtype=np.int32)\n",
        "        else:\n",
        "            return np.insert(np.array(list(product([0,3], repeat = order-1))),0,3,axis=1)\n",
        "\n",
        "    def __k(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[3]])\n",
        "        else:\n",
        "            # print(self.kprime)\n",
        "            # return np.concatenate((self.kprime,self.aprime,self.kprime),axis=0)\n",
        "            return np.concatenate((self.__kprime(order),self.__aprime(order),self.__kprime(order)),axis=0)\n",
        "\n",
        "    def __a(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[1]])\n",
        "        else:\n",
        "            return np.insert(np.array(list(product([0,3], repeat = order-1))),0,1,axis=1)\n",
        "\n",
        "    def __generator(self,order):\n",
        "        if order == 1:\n",
        "            return np.array([[1],[3]])\n",
        "        else:\n",
        "            ret = np.insert(self.__generator(order-1),0,0,axis=1)\n",
        "            return np.concatenate((ret,self.__a(order),self.__aprime(order)),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p2imblDKC33u"
      },
      "outputs": [],
      "source": [
        "def generate_first_kind(eu):\n",
        "  '''\n",
        "  generates a closure relating canonical coordinates of the first kind to an element in SU(2^n) with respect to sigma strings.\n",
        "  The argument is an instance of the Euler class.\n",
        "  '''\n",
        "  eu_fund = tf.constant(np.array([tf.reshape(Sigma(*s).fundamental,[2**eu.order,2**eu.order]) for s in eu.basis]),dtype=tf.complex128)\n",
        "  dim = len(eu.basis)\n",
        "  \n",
        "  @tf.function\n",
        "  def first_kind(theta):\n",
        "    '''\n",
        "    returns an element of SU(2^n) from given canonical coordinates of the first kind whose generators are sigma strings.\n",
        "    '''\n",
        "    theta = tf.cast(theta,tf.complex128)\n",
        "    return tf.linalg.expm(tf.tensordot(theta,eu_fund,axes=1))\n",
        "  \n",
        "  return first_kind\n",
        "\n",
        "def generate_second_kind(eu):\n",
        "  '''\n",
        "  generates a closure relating canonical coordinates of the second kind to an element in SU(2^n) with respect to sigma strings.\n",
        "  The argument is an instance of the Euler class.\n",
        "  '''\n",
        "  eu_fund = tf.constant(np.array([tf.reshape(Sigma(*s).fundamental,[2**eu.order,2**eu.order]) for s in eu.basis]),dtype=tf.complex128);\n",
        "  dim = len(eu.basis)\n",
        "  identity = tf.eye(2**eu.order,dtype=tf.complex128)\n",
        "  @tf.function\n",
        "  def second_kind(theta):\n",
        "    '''\n",
        "    returns an element of SU(2^n) from given canonical coordinates of the second kind whose generators are sigma strings.\n",
        "    '''\n",
        "    c = tf.cast(tf.cos(theta/2),tf.complex128)\n",
        "    s = tf.cast(2*tf.sin(theta/2),tf.complex128)\n",
        "    ret = c[0] * identity + s[0] * eu_fund[0]\n",
        "    for i in range(1,dim):\n",
        "      ret = tf.matmul(ret,(c[i] * identity + s[i] * eu_fund[i]))\n",
        "    return ret\n",
        "  return second_kind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kBCUODKLjycP"
      },
      "outputs": [],
      "source": [
        "def diff_sum(mat1,mat2):\n",
        "  '''\n",
        "  sums over the absolute square elementwise difference between two complex matrices.\n",
        "  '''\n",
        "  diff = mat1 - mat2\n",
        "  return tf.cast(tf.reduce_sum(diff*tf.math.conj(diff)),tf.float64)\n",
        "\n",
        "def generate_diff_sum_grad_fk(mat):\n",
        "  '''\n",
        "  generates a function closure computing the diff_sum for a first kind matrix and a complex matrix as well as its gradient.\n",
        "  '''\n",
        "  order = tf.cast(tf.experimental.numpy.log2(tf.cast(mat.shape[0],tf.float32)),tf.int16)\n",
        "  eu = Euler(order)\n",
        "  fk = generate_first_kind(eu)\n",
        "\n",
        "  def diff_sum_grad_fk(theta):\n",
        "    '''\n",
        "    returns the diff_sum of a first kind matrix and a complex matrix with its gradient.\n",
        "    The first element of the return is the gradient and the last element is the value of diff_sum.\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(theta)\n",
        "      o = diff_sum(mat,fk(theta))\n",
        "\n",
        "    return tape.gradient(o,theta),o\n",
        "\n",
        "  return diff_sum_grad_fk\n",
        "\n",
        "def generate_diff_sum_grad_sk(mat):\n",
        "  '''\n",
        "  generates a function closure computing the diff_sum for a second kind matrix and a complex matrix as well as its gradient.\n",
        "  '''\n",
        "  order = tf.cast(tf.experimental.numpy.log2(tf.cast(mat.shape[0],tf.float32)),tf.int16)\n",
        "  eu = Euler(order)\n",
        "  sk = generate_second_kind(eu)\n",
        "  def diff_sum_grad_sk(theta):\n",
        "    '''\n",
        "    returns the diff_sum of a second kind matrix and a complex matrix with its gradient.\n",
        "    The first element of the return is the gradient and the last element is the value of diff_sum.\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(theta)\n",
        "      o = diff_sum(mat,sk(theta))\n",
        "\n",
        "    return tape.gradient(o,theta),o\n",
        "\n",
        "  return diff_sum_grad_sk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U0iQL07Cp4pL"
      },
      "outputs": [],
      "source": [
        "# Optimization begins.\n",
        "def generate_obj_fk(mat):\n",
        "  '''\n",
        "  generates an objective function for the optimization problem to find the canonical coordinates of the first kind based on the diff_sum.\n",
        "  '''\n",
        "  order = tf.cast(tf.experimental.numpy.log2(tf.cast(mat.shape[0],tf.float32)),tf.int16)\n",
        "  eu = Euler(order)\n",
        "  fk_grad = generate_diff_sum_grad_fk(mat)\n",
        "  def obj_fk(x,grad):\n",
        "    tf_x = tf.constant(x,dtype=tf.float64)\n",
        "    if grad.size > 0:\n",
        "      obj = fk_grad(tf_x)\n",
        "      for i in range(grad.size):\n",
        "        grad[i] = obj[0].numpy()[i]\n",
        "\n",
        "    return obj[1].numpy()\n",
        "\n",
        "  return obj_fk\n",
        "\n",
        "def generate_obj_sk(mat):\n",
        "  '''\n",
        "  generates an objective function for the optimization problem to find the canonical coordinates of the first kind based on the diff_sum.\n",
        "  '''\n",
        "  order = tf.cast(tf.experimental.numpy.log2(tf.cast(mat.shape[0],tf.float32)),tf.int16)\n",
        "  eu = Euler(order)\n",
        "  sk_grad = generate_diff_sum_grad_sk(mat)\n",
        "  def obj_sk(x,grad):\n",
        "    tf_x = tf.constant(x,dtype=tf.float64)\n",
        "    if grad.size > 0:\n",
        "      obj = sk_grad(tf_x)\n",
        "      for i in range(grad.size):\n",
        "        grad[i] = obj[0].numpy()[i]\n",
        "\n",
        "    return obj[1].numpy()\n",
        "\n",
        "  return obj_sk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lB_hG149ItgQ"
      },
      "outputs": [],
      "source": [
        "# unitary matrices to be coordinated\n",
        "h_gate = tf.cast(tf.constant([[1.,1.],[1.,-1.]])/tf.sqrt(2.),dtype=tf.complex128); # Hadarmard gate\n",
        "s_gate = tf.constant([[1,0],[0,-1.0j]],dtype=tf.complex128); # S gate\n",
        "t_gate = tf.constant(np.array([[1,0],[0,np.exp(1.0j*np.pi/8)]],dtype=np.complex128)); # T gate\n",
        "cnot_gate = tf.constant(np.array([[1,0,0,0],[0,1,0,0],[0,0,0,1],[0,0,1,0]],dtype=np.complex128)); #CNOT gate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OzPGtZNMtxdU"
      },
      "outputs": [],
      "source": [
        "unitary_target = h_gate\n",
        "dim = tf.reduce_prod(unitary_target.shape).numpy().astype(int).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 알고리즘 실행(최적화)\n",
        "##### 아래의 코드들은 결국 initial value(np.ones(dim))의 matrix를 만들고 싶어하는 unitary matrix(h_gate, s_gate)과의 차이를 줄이는 방향으로 최적화 하여 그 matrix를 구하고 싶어합니다.(또한 그에 따른 eigenvector와 eigenvalue 역시 구하고 싶어 합니다.) nlopt의 opt 객체를 생성하여 그 opt 객체 내의 set_min_objective 메서드를 이용해 이를 최적화합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX0pg66k1pZl",
        "outputId": "0ebde526-f1e4-4bb6-867e-f43ac6993e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimum at [ 3.14159265e+00  2.22144147e+00 -7.51315101e-11  2.22144147e+00]\n",
            "the objective function converges to 5.858023705462033e-16\n",
            "optimization result:  1\n"
          ]
        }
      ],
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) # quiets Tensorflow warnings.\n",
        "obj_fk = generate_obj_fk(unitary_target) # generates an objective function for a target unitary matrix.\n",
        "opt = nlopt.opt(nlopt.LD_LBFGS,dim) # chooses an optimization algorithm. # LBFGS 알고리즘을 이용해 최적화를 실행합니다.\n",
        "opt.set_min_objective(obj_fk) # sets the objective function for the chosen optimization algorithm.  #opt 인스턴스의 최적화 함수를 설정합니다.\n",
        "opt.set_xtol_rel(1e-5) # sets the tolerance for the convergence.    #최적화 tolerance를 설정합니다.\n",
        "theta_opt_fk = opt.optimize(np.ones(dim)) # sets the initial varaibles.     #초기 값을 설정합니다.\n",
        "obj_final = opt.last_optimum_value() # assigns the variables of the last iteration.\n",
        "# prints results of the optimization algorithm\n",
        "print(\"minimum at\",theta_opt_fk)\n",
        "print(\"the objective function converges to\", obj_final)\n",
        "print(\"optimization result: \",opt.last_optimize_result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlAzaP1WteLV",
        "outputId": "5116dd92-7ba7-453f-8977-0051eda593a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimum at [3.14159265 1.57079632 1.5707963  1.57079632 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "the objective function converges to 9.110270824146652e-16\n",
            "optimization result:  4\n"
          ]
        }
      ],
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) # quiets Tensorflow warnings.\n",
        "obj_sk = generate_obj_sk(h_gate) # generates an objective function for a target unitary matrix.\n",
        "opt = nlopt.opt(nlopt.LD_LBFGS,16) # chooses an optimization algorithm.\n",
        "opt.set_min_objective(obj_sk) # sets the objective function for the chosen optimization algorithm.\n",
        "opt.set_xtol_rel(1e-5) # sets the tolerance for the convergence.\n",
        "theta_opt_sk = opt.optimize(np.ones(16)) # sets the initial varaibles.\n",
        "obj_final = opt.last_optimum_value() # assigns the variables of the last iteration.\n",
        "# prints results of the optimization algorithm\n",
        "print(\"minimum at\",theta_opt_sk)\n",
        "print(\"the objective function converges to\", obj_final)\n",
        "print(\"optimization result: \",opt.last_optimize_result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_EeESNhAq8_D"
      },
      "outputs": [],
      "source": [
        "# validation\n",
        "eu1 = Euler(1)\n",
        "eu2 = Euler(2)\n",
        "fk_mat_su2 = generate_first_kind(eu1)\n",
        "fk_mat_su4 = generate_first_kind(eu2)\n",
        "sk_mat_su2 = generate_second_kind(eu1)\n",
        "sk_mat_su4 = generate_second_kind(eu2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpWwGAajr-FK",
        "outputId": "11a25340-debc-49d4-f5af-6fcc145439d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=complex128, numpy=\n",
              "array([[ 0.70710678+2.39601672e-11j,  0.70710678+9.63573665e-12j],\n",
              "       [ 0.70710678+5.74658654e-11j, -0.70710678-4.31413794e-11j]])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fk_mat_su2(theta_opt_fk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "prb9N3cAjUGI"
      },
      "outputs": [],
      "source": [
        "unitary_target = cnot_gate\n",
        "dim = tf.reduce_prod(unitary_target.shape).numpy().astype(int).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVCaeeq9j8aG",
        "outputId": "35dcdb8c-ed2f-4220-95c5-0c39dd0ec176"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nV0xZMuj88F",
        "outputId": "e732f26c-9b22-4891-a897-6942aaf2e34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimum at [ 1.57079634e+00  1.57079631e+00  7.08445492e-09  7.08445507e-09\n",
            "  7.08445500e-09 -1.69169595e-08 -2.08570818e-09 -2.32231501e-08\n",
            "  7.08445500e-09 -1.69169595e-08 -2.32231501e-08 -2.08570818e-09\n",
            "  1.57079631e+00  1.57079629e+00 -1.69169596e-08 -1.69169598e-08]\n",
            "the objective function converges to 3.436238964984065e-15\n",
            "optimization result:  4\n"
          ]
        }
      ],
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) # quiets Tensorflow warnings.\n",
        "obj_fk = generate_obj_fk(unitary_target) # generates an objective function for a target unitary matrix.\n",
        "opt = nlopt.opt(nlopt.LD_LBFGS,dim) # chooses an optimization algorithm.\n",
        "opt.set_min_objective(obj_fk) # sets the objective function for the chosen optimization algorithm.\n",
        "opt.set_xtol_rel(1e-5) # sets the tolerance for the convergence.\n",
        "theta_opt_fk = opt.optimize(np.ones(dim)) # sets the initial varaibles.\n",
        "obj_final = opt.last_optimum_value() # assigns the variables of the last iteration.\n",
        "# prints results of the optimization algorithm\n",
        "print(\"minimum at\",theta_opt_fk)\n",
        "print(\"the objective function converges to\", obj_final)\n",
        "print(\"optimization result: \",opt.last_optimize_result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9PEqY2xkBj5",
        "outputId": "bd254eb5-794d-4c7c-a558-1c261bac772e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimum at [3.14159265 1.57079632 1.5707963  1.57079632 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.        ]\n",
            "the objective function converges to 9.110270824146652e-16\n",
            "optimization result:  4\n"
          ]
        }
      ],
      "source": [
        "logging.getLogger('tensorflow').setLevel(logging.ERROR) # quiets Tensorflow warnings.\n",
        "obj_sk = generate_obj_sk(h_gate) # generates an objective function for a target unitary matrix.\n",
        "opt = nlopt.opt(nlopt.LD_LBFGS,16) # chooses an optimization algorithm.\n",
        "opt.set_min_objective(obj_sk) # sets the objective function for the chosen optimization algorithm.\n",
        "opt.set_xtol_rel(1e-5) # sets the tolerance for the convergence.\n",
        "theta_opt_sk = opt.optimize(np.ones(16)) # sets the initial varaibles.\n",
        "obj_final = opt.last_optimum_value() # assigns the variables of the last iteration.\n",
        "# prints results of the optimization algorithm\n",
        "print(\"minimum at\",theta_opt_sk)\n",
        "print(\"the objective function converges to\", obj_final)\n",
        "print(\"optimization result: \",opt.last_optimize_result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2ldENufAkJLd"
      },
      "outputs": [],
      "source": [
        "# validation\n",
        "eu1 = Euler(2)\n",
        "eu2 = Euler(4)\n",
        "fk_mat_su2 = generate_first_kind(eu1)\n",
        "fk_mat_su4 = generate_first_kind(eu2)\n",
        "sk_mat_su2 = generate_second_kind(eu1)\n",
        "sk_mat_su4 = generate_second_kind(eu2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Tjeh4ikV4r",
        "outputId": "d1c7fcc5-a351-4e64-c038-abaefd65d1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.-0.j, -0.+0.j, -0.+0.j, -0.+0.j],\n",
              "       [ 0.+0.j,  1.-0.j,  0.+0.j, -0.+0.j],\n",
              "       [ 0.+0.j,  0.+0.j, -0.-0.j,  1.-0.j],\n",
              "       [ 0.+0.j, -0.+0.j,  1.+0.j,  0.-0.j]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.round(fk_mat_su2(theta_opt_fk),6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsOvKHWpkZL6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
